{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from difflib import SequenceMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_similarity_scores(model_type, split):\n",
    "\n",
    "    scenario_scores = {}\n",
    "\n",
    "    result_dir = f'../experiments/repo_eval/repo-eval-350m-{model_type}/trained/functions'\n",
    "\n",
    "    for scenario in os.listdir(result_dir):\n",
    "        if scenario == 'result.jsonl':\n",
    "            continue\n",
    "\n",
    "        # read in the expected code\n",
    "        with open(f'../custom_data_eval/{split}/{scenario}/completed.py') as infile:\n",
    "            expected_output = infile.read()\n",
    "\n",
    "        # read each generated code example for the scenario\n",
    "        outputs = []\n",
    "        if model_type == 'prefix':\n",
    "            scenario_output_dir = f\"{result_dir}/{scenario}/in_repo_output\"\n",
    "        elif model_type == 'lm':\n",
    "            scenario_output_dir = f\"{result_dir}/{scenario}/orig_output\"\n",
    "            \n",
    "        for result in os.listdir(scenario_output_dir):\n",
    "            with open(f'{scenario_output_dir}/{result}') as res_file:\n",
    "                outputs.append(res_file.read())\n",
    "\n",
    "        # calculate scores\n",
    "        min_score = 1\n",
    "        max_score = 0\n",
    "        scores = []\n",
    "        for completion in outputs:\n",
    "            score = SequenceMatcher(None, expected_output, completion).ratio()\n",
    "            scores.append(score)\n",
    "\n",
    "            if score < min_score:\n",
    "                min_score = score\n",
    "            if score > max_score:\n",
    "                max_score = score\n",
    "    \n",
    "        # sometimes we get no comlpetions\n",
    "        if len(scores) != 0:\n",
    "            avg_score = sum(scores)/len(scores)\n",
    "\n",
    "            scenario_scores[scenario] = {\n",
    "                'min_score': min_score, \n",
    "                'max_score': max_score,\n",
    "                'avg_score': avg_score\n",
    "                }\n",
    "\n",
    "    return scenario_scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'example2': {'min_score': 0.7185840707964601,\n",
       "  'max_score': 0.8115384615384615,\n",
       "  'avg_score': 0.7577841036726565},\n",
       " 'example1': {'min_score': 0.12418300653594772,\n",
       "  'max_score': 0.47843137254901963,\n",
       "  'avg_score': 0.3803422237642011},\n",
       " 'example3': {'min_score': 0.4178049929345266,\n",
       "  'max_score': 0.5957805907172996,\n",
       "  'avg_score': 0.4850626251830469},\n",
       " 'example4': {'min_score': 0.43316831683168316,\n",
       "  'max_score': 0.8720626631853786,\n",
       "  'avg_score': 0.7754845860184766}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split = 'test'\n",
    "model_type = 'prefix'\n",
    "\n",
    "get_similarity_scores(model_type, split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'example2': {'min_score': 0.6197183098591549,\n",
       "  'max_score': 0.9140625,\n",
       "  'avg_score': 0.7479658035898037},\n",
       " 'example1': {'min_score': 0.06841686555290374,\n",
       "  'max_score': 0.48725212464589235,\n",
       "  'avg_score': 0.24105947833959585},\n",
       " 'example3': {'min_score': 0.42092746730083236,\n",
       "  'max_score': 0.505175983436853,\n",
       "  'avg_score': 0.46549576853199454},\n",
       " 'example4': {'min_score': 0.59375,\n",
       "  'max_score': 0.8493333333333334,\n",
       "  'avg_score': 0.797072631689971}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split = 'test'\n",
    "model_type = 'lm'\n",
    "\n",
    "get_similarity_scores(model_type, split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
